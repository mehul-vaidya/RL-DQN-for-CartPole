# DQN- Deep Q Network (Q Learning + Neural Network)
- This project implements a Deep Q-Network (DQN) agent using PyTorch to learn the CartPole-v1 environment
through trial and error. The agent uses experience replay and a target network to stabilize learning 
while balancing exploration and exploitation. Over multiple episodes, the neural network learns a 
policy that attempts to keep the pole balanced as long as possible.

# Double DQN -
- In DQN The same network: chooses the best action  and evaluates that action
- next_q = target_net(next_states).max(1)[0]
- In double DQN
- Policy Net â†’ chooses best action
- Target Net â†’ evaluates that action
## Action selection using policy_net
- next_actions = torch.argmax(policy_net(next_states), dim=1, keepdim=True)
## Action evaluation using target_net
- next_q = target_net(next_states).gather(1, next_actions)

![img.png](img.png)

## DQN Explination in terms of DOG getting trained. 
- We create a dog with a brain (neural network) and put it in a game (CartPole).
- The dog tries actions, gets rewards, remembers what happened, and slowly updates its brain.
- One brain learns, another brain gives stable advice.
- Over many attempts, the dog learns how to balance the pole without falling.

## Create the Dogâ€™s Brain (Neural Network)
- class DQN(nn.Module):

## Create the World (Environment)
- env = gym.make("CartPole-v1")

## Create TWO Brains
- policy_net  # learning brain
- target_net  # teaching brain

## Set Learning Rules (Hyperparameters)
- These are training rules:
- epsilon â†’ how curious the dog is
- gamma â†’ how much future rewards matter
- memory â†’ diary of past experiences
- batch_size â†’ how many memories to learn from
- episodes â†’ how many training days

## Action Selection (Explore vs Exploit)
- select_action(state)

## Experience Storage (Memory)
- store_experience(s, a, r, s_next, done)

## Learning Step (train_step)
- This is where real learning happens.

## Training Loop (Daily Practice)
- Each episode = one full attempt.

## Slowly Reduce Randomness
- â€œIâ€™ve learned enough. Iâ€™ll trust my brain more now.â€

## Update Teacher Brain Occasionally
- target_net.load_state_dict(policy_net.state_dict())


# PPO (Proximal Policy Optimization)
- 1ï¸âƒ£ Two Brains in One Model
- ğŸ§  Actor (Decision Maker)
- â€œWhich action should I take?â€
- Outputs probabilities, not fixed actions.
- ğŸ§  Critic (Judge)
- â€œHow good is my current situation?â€
- Tells if the dog is doing well or badly.
- No Replay Buffer (Big Difference!)
- PPO learns from recent experiences only
- Train â†’ discard â†’ collect new data

# PPO explaination 
- ğŸ§  Explain PPO in SIMPLE Terms

## Think of a dog learning to balance a stick ğŸ•
- 1ï¸âƒ£ Two Brains in One Model
- ğŸ§  Actor (Decision Maker)

## â€œWhich action should I take?â€
- Outputs probabilities, not fixed actions.

## ğŸ§  Critic (Judge)
- â€œHow good is my current situation?â€
- Tells if the dog is doing well or badly.

## 2ï¸âƒ£ No Replay Buffer (Big Difference!)
- PPO learns from recent experiences only
- Train â†’ discard â†’ collect new data
- This keeps learning stable and safe

## 3ï¸âƒ£ Action Selection
- dist = Categorical(probs)
- action = dist.sample()

## ğŸ§  Meaning:
- Donâ€™t always pick the best move
- Try good moves more often
- Try bad moves less often

## 4ï¸âƒ£ Advantage (Was it Worth It?)
- Advantage = Actual reward âˆ’ Criticâ€™s expectation

## ğŸ• Interpretation:
- Positive â†’ â€œNice! Do this moreâ€
- Negative â†’ â€œOopsâ€¦ avoid thisâ€

## #ï¸âƒ£ PPOâ€™s Special Trick (Clipping)
- ratio = new_policy / old_policy
- clip(ratio, 0.8, 1.2)

## ğŸ• Meaning:
- â€œImproveâ€¦ but donâ€™t change too much at onceâ€
- This is why PPO is stable.

## 6ï¸âƒ£ Training Flow (Very Simple)
- Play episode
- Store states & rewards
- Calculate advantage
- Update policy safely (PPO)
- Forget old data
- Repeat

## 7ï¸âƒ£ Testing
- action = argmax(probs)

## ğŸ¯ No randomness
## ğŸ¯ Just best learned behavior

## ğŸ§  Why PPO Works Better Than DQN Here
- Problem	PPO
- Overestimation	âŒ
- Target network	âŒ
- Îµ tuning	âŒ
- Stability	âœ…
- Continuous actions	âœ…
